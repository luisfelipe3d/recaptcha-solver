{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7307a1f3-e20b-4bea-a80e-a4e4354c6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a63b29-9c4c-4e99-8adc-1f6e37c65662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f32f2ae4-3e44-42e6-8685-5e2fd7b75201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.4526, 0.4443, 0.4526])\n",
      "Std: tensor([0.2100, 0.2125, 0.2112])\n"
     ]
    }
   ],
   "source": [
    "# Caminho para o seu dataset\n",
    "data_dir = './editedDataset'\n",
    "\n",
    "# Transformação para converter as imagens em tensores\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Redimensiona as imagens (ajuste conforme necessário)\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Carregar o dataset\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# DataLoader para iterar pelo dataset\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "def calculate_mean_std(dataloader):\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_images = 0\n",
    "\n",
    "    for images, _ in dataloader:\n",
    "        # Redimensiona o batch: (batch_size, 3, height, width) para (3, batch_size*height*width)\n",
    "        images = images.view(3, -1)\n",
    "        \n",
    "        # Calcula a média e soma\n",
    "        mean += images.mean(1)\n",
    "        \n",
    "        # Calcula o desvio padrão e soma\n",
    "        std += images.std(1)\n",
    "        \n",
    "        total_images += 1\n",
    "\n",
    "    # Média total\n",
    "    mean /= total_images\n",
    "    \n",
    "    # Desvio padrão total\n",
    "    std /= total_images\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "# Calcula a média e o desvio padrão\n",
    "mean, std = calculate_mean_std(dataloader)\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6b635e-4d31-4c12-ad11-e38aa04b76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_dir, batch_size, random_seed=42, train_size=0.8, valid_size=0.1, shuffle=True):\n",
    "    # Define as transformações\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4526, 0.4443, 0.4526],  # Ajuste esses valores para o seu dataset\n",
    "        std=[0.2100, 0.2125, 0.2112],\n",
    "    )\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    # Carregar o dataset\n",
    "    dataset = datasets.ImageFolder(\n",
    "        root=data_dir,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    num_images = len(dataset)\n",
    "    print(f\"Total de imagens no dataset: {num_images}\")\n",
    "\n",
    "    num_train = len(dataset)\n",
    "    indices = list(range(num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    # Divisão dos índices\n",
    "    train_split = int(np.floor(train_size * num_train))\n",
    "    valid_split = int(np.floor((train_size + valid_size) * num_train))\n",
    "\n",
    "    train_idx, valid_idx, test_idx = indices[:train_split], indices[train_split:valid_split], indices[valid_split:]\n",
    "\n",
    "    # Criar samplers\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    # Criar data loaders\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    print(f\"Imagens de treinamento: {len(train_idx)}\")\n",
    "    print(f\"Imagens de validação: {len(valid_idx)}\")\n",
    "    print(f\"Imagens de teste: {len(test_idx)}\")\n",
    "\n",
    "    return (train_loader, valid_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7d0d1bb-36f9-4394-b275-d6b34c31258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de imagens no dataset: 4194\n",
      "Imagens de treinamento: 3355\n",
      "Imagens de validação: 419\n",
      "Imagens de teste: 420\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "train_size= 0.8\n",
    "valid_size = 0.1\n",
    "\n",
    "train_loader, valid_loader, test_loader = data_loader(data_dir=data_dir, batch_size=batch_size, \n",
    "                                                      train_size=train_size, valid_size=valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e04dc4-2905-4ee9-bc2c-3acd82d8a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = self._make_layers()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layers(self):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        \n",
    "        # Configuração da arquitetura VGG16\n",
    "        cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, v, kernel_size=3, padding=1),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "                \n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd906cf1-4f24-43cc-8aeb-252435e23567",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001 # for Adam 0.0001 or 0.0004\n",
    "weight_decay = 0.005\n",
    "momentum = 0.9\n",
    "\n",
    "model = VGG16(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "classes = ['Bicycle', 'Bridge', 'Bus', 'Car', 'Traffic Light']\n",
    "class_correct = list(0. for i in range(num_classes))\n",
    "class_total = list(0. for i in range(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b00f2e2-609e-4b4e-ab5e-f6f2e0774aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [105/105], Loss: 1.575406\n",
      "Accuracy of the network on the validation images: 27.207637 %\n",
      "Accuracy of the network on the test images: 24.761905 %\n",
      "Precision: 0.0613, Recall: 0.2476, F1-Score: 0.0983\n",
      "Epoch [2/10], Step [105/105], Loss: 1.604260\n",
      "Accuracy of the network on the validation images: 19.809069 %\n",
      "Accuracy of the network on the test images: 25.476190 %\n",
      "Precision: 0.0649, Recall: 0.2548, F1-Score: 0.1035\n",
      "Epoch [3/10], Step [105/105], Loss: 1.437171\n",
      "Accuracy of the network on the validation images: 35.083532 %\n",
      "Accuracy of the network on the test images: 33.809524 %\n",
      "Precision: 0.2090, Recall: 0.3381, F1-Score: 0.2073\n",
      "Epoch [4/10], Step [105/105], Loss: 1.175718\n",
      "Accuracy of the network on the validation images: 40.811456 %\n",
      "Accuracy of the network on the test images: 40.952381 %\n",
      "Precision: 0.3824, Recall: 0.4095, F1-Score: 0.3659\n",
      "Epoch [5/10], Step [105/105], Loss: 1.366759\n",
      "Accuracy of the network on the validation images: 45.823389 %\n",
      "Accuracy of the network on the test images: 45.476190 %\n",
      "Precision: 0.4290, Recall: 0.4548, F1-Score: 0.4142\n",
      "Epoch [6/10], Step [105/105], Loss: 1.037647\n",
      "Accuracy of the network on the validation images: 50.596659 %\n",
      "Accuracy of the network on the test images: 49.285714 %\n",
      "Precision: 0.4954, Recall: 0.4929, F1-Score: 0.4798\n",
      "Epoch [7/10], Step [105/105], Loss: 1.325245\n",
      "Accuracy of the network on the validation images: 50.357995 %\n",
      "Accuracy of the network on the test images: 60.238095 %\n",
      "Precision: 0.6106, Recall: 0.6024, F1-Score: 0.5871\n",
      "Epoch [8/10], Step [105/105], Loss: 1.319483\n",
      "Accuracy of the network on the validation images: 52.983294 %\n",
      "Accuracy of the network on the test images: 61.428571 %\n",
      "Precision: 0.6280, Recall: 0.6143, F1-Score: 0.6155\n",
      "Epoch [9/10], Step [105/105], Loss: 0.903423\n",
      "Accuracy of the network on the validation images: 57.517900 %\n",
      "Accuracy of the network on the test images: 64.047619 %\n",
      "Precision: 0.6764, Recall: 0.6405, F1-Score: 0.6228\n",
      "Epoch [10/10], Step [105/105], Loss: 1.018632\n",
      "Accuracy of the network on the validation images: 61.097852 %\n",
      "Accuracy of the network on the test images: 65.000000 %\n",
      "Precision: 0.6787, Recall: 0.6500, F1-Score: 0.6564\n",
      "Total training time: 267.850246 seconds.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.6f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Validation loop\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        print('Accuracy of the network on the validation images: {:.6f} %'.format(100 * correct / total))\n",
    "\n",
    "    # Testing loop\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        print('Accuracy of the network on the test images: {:.6f} %'.format(100 * correct / total))\n",
    "\n",
    "    # Calcula as métricas com tratamento de divisão por zero\n",
    "    precision = precision_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n",
    "\n",
    "# Synchronize after training completion if using CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "end_time = time.time()\n",
    "total_training_time = end_time - start_time\n",
    "\n",
    "print(f\"Total training time: {total_training_time:.6f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41c30408-5d06-4a0b-a0d1-ffb7f63dfead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "           Linear-32                 [-1, 4096]     102,764,544\n",
      "             ReLU-33                 [-1, 4096]               0\n",
      "          Dropout-34                 [-1, 4096]               0\n",
      "           Linear-35                 [-1, 4096]      16,781,312\n",
      "             ReLU-36                 [-1, 4096]               0\n",
      "          Dropout-37                 [-1, 4096]               0\n",
      "           Linear-38                    [-1, 5]          20,485\n",
      "================================================================\n",
      "Total params: 134,281,029\n",
      "Trainable params: 134,281,029\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.58\n",
      "Params size (MB): 512.24\n",
      "Estimated Total Size (MB): 731.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('VGG16 Summary:')\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d483c10-5551-452e-a872-70edce654e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
